## The main decision
How exactly will the encoding work.
For this to be fulfil the goal of taking less space then UTF-8 we need to use less then 8 bits per character. So using 6 bits might be a cheap and easy way answer since I only really need a-z, A-Z and 0-9 with a small set of special characters like space and dashes. So from some basic work in the calculator I can find out that a-z is 26 and A-Z is also 26 characters, 0-9 is 10 characters and that results in 62 which leaves space for my space and a dash character, resulting in our now perfect Not-UTF-6. However, is it this simple?
## Not-UTF-5
So can we get away cheaper? A 5 bit width would be able to hold 32 characters. Theoretically if we forbid upper-case letters and only translate the original 26 characters, aka not store upper or lower and let the user display the characters as upper or lower, we could get by with only 5 bits and 6 symbols left to fill out encoding with. However, we could make it so that one of the last 6 symbols is a shift symbol that just makes the next character uppercase when read. But this would mean that uppercase characters will take up 10 bits effectively, which might be okay depending on how often we use uppercase letters. Or a caps-lock type symbol instead that toggle the reader into reading everything as upper-case until a "caps-lock" symbol is read again.
## Not-UTF-4
We could fit our 26 lowercase letters in a 4 bit width by doing some more shenanigan-theory. By making it so that the first 8 representation we can get out of the 3 first bits holds the 8 first letters and then the 4th bit would be a flag to notify the reader that we will need the next symbol to represent our character. This mean that files with a lot of 'a' characters could possible be half the size in the best case. In the worst case we will need to use 16 bits to make a bunch of 'z' character filled files... This could quickly nullify any advantage over normal UTF-8. However there is still hope for our 4 bit friend. It would be possible to make a shared and more dynamic encoding that would on write produce the 4 bit width encoded file and a configuration file. The configuration file would be generated on write and be a sorted list of the source file's character usage in order of most to least prevalent. The configuration file would then be used on write with the most prevalent characters in the first 8 representations and the least prevalent characters in the third set of 8 representations. And then this configuration will be used on read to produce the original file. This would effectively mean that the file is encrypted and the configuration is a per-file basis key to the file. To make it so every picture does not need it's own personalized configuration file, we might have to make it project wide or at least generate over a large set of files or folders. This dynamic encoding would also help mitigate the wastage of bits by having the most used characters cheap to store. Still, worst case scenario we will need more bits then standard UTF-8. This whole document has made me realize more and more why UTF-8 is made the way it is.
## Not-UTF-3
There will sadly not be any theoretical "Not-UTF-3" :(
